# edubert

Readme to be updated with instructions very soon.

## Using the models

Both versions of EduBERT were trained using [Transformers by Huggingface](https://github.com/huggingface/transformers). The best way to use the model is to load its weights&config through the PyTorch implementation of Transformers.

EduBERT (.tar.gz, ±388.7MB): https://storage.googleapis.com/edubert/edubert.tar.gz

DistilEduBERT (.tar.gz, ±235.5MB): https://storage.googleapis.com/edubert/distiledubert.tar.gz


## Citing

arXiv (for now):
```
@article{clavie2019edubert,
  title={EduBERT: Pretrained Deep Language Models for Learning Analytics},
  author={Clavi{\'e}, Benjamin and Gal, Kobi},
  journal={arXiv preprint arXiv:1912.00690, to be presented at the Tenth International Conference on Learning Analytics And Knowledge - LAK 20},
  year={2019}
}
```

Conference Proceedings (pending):
```
@inproceedings{edubert,
title={EduBERT: Pretrained Deep Language Models for Learning Analytics},
booktitle={Companion Proceedings of the Tenth International Conference on Learning Analytics And Knowledge - LAK 20},
author={Clavié, Benjamin and Gal, Kobi},
year={2020}}
```
